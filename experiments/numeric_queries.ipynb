{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b411df8ca2e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "import string\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as f\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, '../src')\n",
    "from tableCollections import TableCollections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName(\"TableCollections\") \\\n",
    "                .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "                .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "                .config(\"spark.rdd.compress\", \"true\") \\\n",
    "                .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkingTable = spark.read.format('csv').options(header='true',inferschema='true').load('/user/ecc290/HW1data/parking-violations-header.csv')\n",
    "openTable = spark.read.format('csv').options(header='true',inferschema='true').load('/user/ecc290/HW1data/open-violations-header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num metadata file exists for table open\n",
      "num metadata file exists for table parking\n",
      "timestamp metadata file exists for table parking\n",
      "+--------------+---------+\n",
      "|       colName|tableName|\n",
      "+--------------+---------+\n",
      "|summons_number|     open|\n",
      "|   fine_amount|     open|\n",
      "|summons_number|  parking|\n",
      "|violation_code|  parking|\n",
      "+--------------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|   colName|tableName|\n",
      "+----------+---------+\n",
      "|issue_date|  parking|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "tc.register(openTable, \"open\")\n",
    "tc.register(parkingTable, \"parking\")\n",
    "tc.numColWithinRange(0, 1000000000000).show()\n",
    "tc.timeColWithinRange(datetime.datetime(1994,1,1), datetime.datetime(2018,5,1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "dirname = '/user/jp4989/open_data_csv/'\n",
    "found = False\n",
    "with open('data_ids', 'r') as g:\n",
    "    ids = g.readlines()\n",
    "    for each in ids:\n",
    "        if each.strip() == '54k3-2wtq':\n",
    "            found = True\n",
    "        if not found:\n",
    "            continue\n",
    "        filename = dirname+each.strip()+'.csv'\n",
    "        df = spark.read.format('csv').options(header='true',inferschema='true').load(filename)\n",
    "        if not df.rdd.isEmpty():\n",
    "            tc.register(df, 'nyc_'+each.strip().replace('-','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "dirname = '/user/jp4989/open_data_csv/'\n",
    "with open('data_ids', 'r') as g:\n",
    "    ids = g.readlines()\n",
    "    for each in ids:\n",
    "        tc.add_registered_table_name('nyc_'+each.strip().replace('-','_'))\n",
    "        if each.strip() == '2n4x-d97d':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             colName|    tableName|\n",
      "+--------------------+-------------+\n",
      "|City Council Dist...|nyc_22rf_yxcy|\n",
      "| Community Districts|nyc_22rf_yxcy|\n",
      "|  Borough Boundaries|nyc_22rf_yxcy|\n",
      "|    Police Precincts|nyc_22rf_yxcy|\n",
      "|         % Level 3+4|nyc_26kp_bgdh|\n",
      "|           % Level 2|nyc_26kp_bgdh|\n",
      "|           % Level 1|nyc_26kp_bgdh|\n",
      "|           % Level 3|nyc_26kp_bgdh|\n",
      "|  Average Class Size|nyc_276h_y36a|\n",
      "|          Level3+4_%|nyc_27h8_t3wt|\n",
      "|            Level2_%|nyc_27h8_t3wt|\n",
      "|            Level3_%|nyc_27h8_t3wt|\n",
      "|            district|nyc_27h8_t3wt|\n",
      "|            Level4_%|nyc_27h8_t3wt|\n",
      "|            Level1_%|nyc_27h8_t3wt|\n",
      "|            latitude|nyc_29km_avyc|\n",
      "|   Pct Level 3 and 4|nyc_2bh6_qmgg|\n",
      "|         Pct Level 3|nyc_2bh6_qmgg|\n",
      "|         Pct Level 4|nyc_2bh6_qmgg|\n",
      "|         Pct Level 2|nyc_2bh6_qmgg|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc.numColWithinRange(0, 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|  colName|    tableName|\n",
      "+---------+-------------+\n",
      "|longitude|nyc_29km_avyc|\n",
      "|Longitude|nyc_2fra_mtpn|\n",
      "|Longitude|nyc_2n4x_d97d|\n",
      "|Longitude|nyc_2q48_ip9a|\n",
      "|  Borough|nyc_35f6_8qd2|\n",
      "|Longitude|nyc_35sw_rdxj|\n",
      "|Longitude|nyc_36hn_wea6|\n",
      "|Longitude|nyc_37fm_7uaa|\n",
      "|Longitude|nyc_37it_gmcp|\n",
      "|Longitude|nyc_3qfc_4tta|\n",
      "|Longitude|nyc_3rfa_3xsf|\n",
      "|Longitude|nyc_3spy_rjpw|\n",
      "|Longitude|nyc_48pb_zy2g|\n",
      "|Longitude|nyc_4wf2_7kdu|\n",
      "|Longitude|nyc_4zdr_zwdi|\n",
      "|Longitude|nyc_56u5_n9sa|\n",
      "|Longitude|nyc_57mv_nv28|\n",
      "|Longitude|nyc_59kj_x8nc|\n",
      "|Longitude|nyc_59t5_r7nb|\n",
      "|Longitude|nyc_5e24_x4wa|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return NYC longitude columns\n",
    "tc.numColWithinRange(-76, -70).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "| colName|    tableName|\n",
      "+--------+-------------+\n",
      "|latitude|nyc_29km_avyc|\n",
      "|Latitude|nyc_2fra_mtpn|\n",
      "|Latitude|nyc_2n4x_d97d|\n",
      "|Latitude|nyc_2q48_ip9a|\n",
      "|Latitude|nyc_35sw_rdxj|\n",
      "|Latitude|nyc_36hn_wea6|\n",
      "|Latitude|nyc_37fm_7uaa|\n",
      "|Latitude|nyc_37it_gmcp|\n",
      "|Latitude|nyc_3qfc_4tta|\n",
      "|Latitude|nyc_3rfa_3xsf|\n",
      "|Latitude|nyc_3spy_rjpw|\n",
      "|Latitude|nyc_48pb_zy2g|\n",
      "|Latitude|nyc_4wf2_7kdu|\n",
      "|Latitude|nyc_4zdr_zwdi|\n",
      "|Latitude|nyc_56u5_n9sa|\n",
      "|Latitude|nyc_57mv_nv28|\n",
      "|Latitude|nyc_59kj_x8nc|\n",
      "|Latitude|nyc_59t5_r7nb|\n",
      "|Latitude|nyc_5e24_x4wa|\n",
      "|Latitude|nyc_5fn4_dr26|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return NYC latitude columns\n",
    "tc.numColWithinRange(39, 43).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|colName|tableName|\n",
      "+-------+---------+\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return columns between 0 and 1\n",
    "tc.numColWithinRange(0, 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             colName|    tableName|\n",
      "+--------------------+-------------+\n",
      "|Unregistered Vehi...|nyc_2bnn_yakx|\n",
      "|COUNT PUBLIC ASSI...|nyc_2t32_hbca|\n",
      "|COUNT CITIZEN STA...|nyc_2t32_hbca|\n",
      "|PERCENT GENDER UN...|nyc_2t32_hbca|\n",
      "|COUNT PACIFIC ISL...|nyc_2t32_hbca|\n",
      "|COUNT AMERICAN IN...|nyc_2t32_hbca|\n",
      "|COUNT GENDER UNKNOWN|nyc_2t32_hbca|\n",
      "|        Derived View|nyc_56b7_s9t5|\n",
      "|Average Minors pe...|nyc_5r5y_pvs3|\n",
      "|            Latitude|nyc_656a_faqy|\n",
      "|PERCENT NRECEIVES...|nyc_6bic_qvek|\n",
      "|PERCENT RECEIVES ...|nyc_6bic_qvek|\n",
      "|PERCENT PUBLIC AS...|nyc_6bic_qvek|\n",
      "|PERCENT PERMANENT...|nyc_6bic_qvek|\n",
      "|COUNT PUBLIC ASSI...|nyc_6bic_qvek|\n",
      "|PERCENT CITIZEN S...|nyc_6bic_qvek|\n",
      "|PERCENT OTHER CIT...|nyc_6bic_qvek|\n",
      "|COUNT CITIZEN STA...|nyc_6bic_qvek|\n",
      "|PERCENT WHITE NON...|nyc_6bic_qvek|\n",
      "|PERCENT ASIAN NON...|nyc_6bic_qvek|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return columns between 0 and 1\n",
    "tc.numColWithinRange(-0.1, 1.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+----------+\n",
      "|    tableName| colName|       min|       max|\n",
      "+-------------+--------+----------+----------+\n",
      "|nyc_2n4x_d97d|Latitude|  40.76135| 40.769233|\n",
      "|nyc_29km_avyc|latitude| 40.508283| 40.906225|\n",
      "|nyc_35f6_8qd2| Borough|-74.124757|-74.022721|\n",
      "+-------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc.getNumRange([\"nyc_2n4x_d97d^Latitude\", \"nyc_29km_avyc^latitude\", \"nyc_35f6_8qd2^Borough\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+-------------------+\n",
      "|    tableName|          colName|                min|                max|\n",
      "+-------------+-----------------+-------------------+-------------------+\n",
      "|nyc_6246_94tp|TCO Obtained Date|2004-01-08 00:00:00|2018-03-14 00:00:00|\n",
      "|nyc_avz8_mqzz| DropOff_datetime|2017-01-02 13:28:00|2018-12-31 00:20:00|\n",
      "|nyc_cwx7_agsh|        StartDate|2006-04-03 00:00:00|2017-06-01 00:00:00|\n",
      "+-------------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc.getTimeRange([\"nyc_6246_94tp^TCO Obtained Date\",  \"nyc_avz8_mqzz^DropOff_datetime\", \"nyc_cwx7_agsh^StartDate\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Given column does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-be021cf5b31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSimilarNumCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a^b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/jp4989/bigdataproject/tableCollections.py\u001b[0m in \u001b[0;36mgetSimilarNumCols\u001b[0;34m(self, tableColName)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtableName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_num_metadata.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Given column does not exist!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableNames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Given column does not exist!"
     ]
    }
   ],
   "source": [
    "tc.getSimilarNumCols(\"a^b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as f\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"colName\", StringType(), True),\n",
    "            StructField(\"min\", DoubleType(), True),\n",
    "            StructField(\"max\", DoubleType(), True)])\n",
    "currentTable = spark.read.csv(\"nyc_d8cz_kh5x_num_metadata.csv\",header=False,schema=schema, sep='^')\n",
    "minMax = currentTable.where(currentTable.colName == \"2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = minMax.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|colName|min|  max|\n",
      "+-------+---+-----+\n",
      "|  Total|5.0|554.0|\n",
      "|   2006|0.0|136.0|\n",
      "|   2009|0.0|110.0|\n",
      "|   2005|0.0|100.0|\n",
      "|   2007|0.0|118.0|\n",
      "|   2008|0.0| 90.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currentTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(max - min)|\n",
      "+-----------+\n",
      "|      549.0|\n",
      "|      136.0|\n",
      "|      110.0|\n",
      "|      100.0|\n",
      "|      118.0|\n",
      "|       90.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currentTable.select(currentTable.max-currentTable.min).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-205030fcfb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurrentTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcurrentTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         raise ValueError(\"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n\u001b[0m\u001b[1;32m    513\u001b[0m                          \"'~' for 'not' when building DataFrame boolean expressions.\")\n\u001b[1;32m    514\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "currentTable.select(min(currentTable.max, 10)-max(currentTable.min,3) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 0.003629764065335753\n",
      "2006 0.029411764705882353\n",
      "2009 0.03636363636363636\n",
      "2005 0.04\n",
      "2007 0.03389830508474576\n",
      "2008 0.044444444444444446\n"
     ]
    }
   ],
   "source": [
    "# 3, 7\n",
    "rows = currentTable.rdd.collect()\n",
    "for each in rows:\n",
    "    intersection = min(each[\"max\"], 7) - max(each[\"min\"], 3)\n",
    "    union = max(each[\"max\"], 7) - min(each[\"min\"], 3)\n",
    "    print(each[\"colName\"], intersection/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-02 17:09:53.071277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a36bf22ed8dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#end: ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSimilarNumCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nyc_2n4x_d97d^Latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/jp4989/bigdataproject/tableCollections.py\u001b[0m in \u001b[0;36mgetSimilarNumCols\u001b[0;34m(self, tableColName)\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0mresultCreated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcase_1_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                         \u001b[0mresultDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresultDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_1_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tableName\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcase_2_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36misEmpty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \"\"\"\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsNewAPIHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python/3.4.4/lib/python3.4/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#start: 4:05pm\n",
    "#end: ?\n",
    "print(datetime.datetime.now())\n",
    "tc.getSimilarNumCols(\"nyc_2n4x_d97d^Latitude\").show() \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-02 17:17:53.960565\n",
      "+--------+-------------+--------------------+\n",
      "| colName|    tableName|                 iou|\n",
      "+--------+-------------+--------------------+\n",
      "|Latitude|nyc_2n4x_d97d|                 1.0|\n",
      "|latitude|nyc_29km_avyc|0.019809419463136916|\n",
      "|Latitude|nyc_2fra_mtpn|0.019043092869228086|\n",
      "+--------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#30 seconds with or without sort, 19 seconds if ignoring no intersection\n",
    "print(datetime.datetime.now())\n",
    "tc.getSimilarNumCols(\"nyc_2n4x_d97d^Latitude\", 0.01).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+\n",
      "|tableName|colName|iou|\n",
      "+---------+-------+---+\n",
      "+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc.getSimilarNumCols(\"nyc_2n4x_d97d^Latitude\", 1.1).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-02 17:12:03.638909\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMax = currentTable.where(currentTable.colName == \"2008\").collect()[0]\n",
    "currentMin = minMax[\"min\"]\n",
    "currentMax = minMax[\"max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|colName|min|  max|\n",
      "+-------+---+-----+\n",
      "|   2006|0.0|136.0|\n",
      "|   2009|0.0|110.0|\n",
      "|   2005|0.0|100.0|\n",
      "|   2007|0.0|118.0|\n",
      "|   2008|0.0| 90.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currentTable.where(currentTable.min <= currentMin).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = currentTable.where(currentTable.min <= currentMin)\n",
    "b = currentTable.where(currentTable.min > currentMin)\n",
    "c = currentTable.where(currentTable.max <= currentMax)\n",
    "d = currentTable.where(currentTable.max > currentMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|colName|min|  max|\n",
      "+-------+---+-----+\n",
      "|   2006|0.0|136.0|\n",
      "|   2009|0.0|110.0|\n",
      "|   2005|0.0|100.0|\n",
      "|   2007|0.0|118.0|\n",
      "|   2008|0.0| 90.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|colName|min|  max|\n",
      "+-------+---+-----+\n",
      "|  Total|5.0|554.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|colName|min| max|\n",
      "+-------+---+----+\n",
      "|   2008|0.0|90.0|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|colName|min|  max|\n",
      "+-------+---+-----+\n",
      "|  Total|5.0|554.0|\n",
      "|   2006|0.0|136.0|\n",
      "|   2009|0.0|110.0|\n",
      "|   2005|0.0|100.0|\n",
      "|   2007|0.0|118.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------+\n",
      "|colName|((max - 0.0) / (90.0 - min))|\n",
      "+-------+----------------------------+\n",
      "|   2008|                         1.0|\n",
      "+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#currentTable.min <= currentMin\n",
    "#currentTable.max <= currentMax\n",
    "intersection = currentTable.max - currentMin\n",
    "union = currentMax - currentTable.min\n",
    "case_1 = a.intersect(c)\n",
    "case_1.select(case_1.colName,(case_1.max-currentMin)/(currentMax-case_1.min)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|colName|(90.0 / (max - min))|\n",
      "+-------+--------------------+\n",
      "|   2005|                 0.9|\n",
      "|   2007|  0.7627118644067796|\n",
      "|   2009|  0.8181818181818182|\n",
      "|   2006|  0.6617647058823529|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#currentTable.min <= currentMin\n",
    "#currentTable.max > currentMax\n",
    "intersection = currentMax - currentMin\n",
    "union = currentTable.max - currentTable.min\n",
    "case_2 = a.intersect(d)\n",
    "case_2.select(case_2.colName,(currentMax - currentMin)/(case_2.max-case_2.min)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|colName|((max - min) / 90.0)|\n",
      "+-------+--------------------+\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#currentTable.min > currentMin\n",
    "#currentTable.max <= currentMax\n",
    "intersection = currentTable.max - currentTable.min\n",
    "union = currentMax - currentMin\n",
    "case_3 = b.intersect(c)\n",
    "case_3.select(case_3.colName,(case_3.max-case_3.min)/(currentMax-currentMin)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|colName|                iou|\n",
      "+-------+-------------------+\n",
      "|  Total|0.15342960288808663|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#currentTable.min > currentMin\n",
    "#currentTable.max > currentMax\n",
    "intersection = currentMax - currentTable.min\n",
    "union = currentTable.max - currentMin\n",
    "case_4 = b.intersect(d)\n",
    "case_4.select(case_4.colName,((currentMax - case_4.min)/(case_4.max-currentMin)).alias(\"iou\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentTable.rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[colName: string, min: double, max: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(sc.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
