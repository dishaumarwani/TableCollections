{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "import string\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as f\n",
    "from collections import defaultdict\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName(\"TableCollections\") \\\n",
    "                .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "                .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "                .config(\"spark.rdd.compress\", \"true\") \\\n",
    "                .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkingTable = spark.read.format('csv').options(header='true',inferschema='true').load('/user/ecc290/HW1data/parking-violations-header.csv')\n",
    "openTable = spark.read.format('csv').options(header='true',inferschema='true').load('/user/ecc290/HW1data/open-violations-header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "import string\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as f\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, StringType\n",
    "\n",
    "class TableCollections:\n",
    "    def __init__(self,spark,sc):\n",
    "        self.spark = spark\n",
    "        self.sc = sc\n",
    "        self.tableNames = []\n",
    "        self.fs = self.sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "        \n",
    "    def add_registered_table_name(self, name):\n",
    "        numFileName = name + \"_num_metadata.csv\"\n",
    "        timeFileName = name + \"_time_metadata.csv\"\n",
    "        stringFileName = name + \"_string_metadata.csv\"\n",
    "        if  self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(numFileName)) or \\\n",
    "            self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(timeFileName)) or \\\n",
    "            self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(stringFileName)):\n",
    "            self.tableNames.append(name)\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def register(self, df, name):\n",
    "        # Clean up column names so that we can prevent future errors\n",
    "        for colName, dtype in df.dtypes:\n",
    "            if '.' in colName or '`' in colName or colName.strip() != colName:\n",
    "                df = df.withColumnRenamed(colName, colName.strip().replace(\".\", \"\").replace(\"`\", \"\"))\n",
    "\n",
    "        # track down which tables have been registered to the class\n",
    "        self.tableNames.append(name)\n",
    "        numFileName = name + \"_num_metadata.csv\"\n",
    "        timeFileName = name + \"_time_metadata.csv\"\n",
    "        stringFileName = name + \"_string_metadata.csv\"\n",
    "        num_cols, time_cols, string_cols, bool_cols = [], [], [], [] #####\n",
    "        df.createOrReplaceTempView(name) # can be problematic\n",
    "\n",
    "        # put column names into appropriate bin\n",
    "        for colName, dtype in df.dtypes:\n",
    "            if dtype == 'timestamp':\n",
    "                time_cols.append(colName)\n",
    "            elif dtype == 'string':\n",
    "                string_cols.append(colName)\n",
    "            elif dtype == 'boolean':#####\n",
    "                bool_cols.append(colName)\n",
    "            else:\n",
    "                num_cols.append(colName)\n",
    "\n",
    "        # For each datatype of columns, process metadata\n",
    "        if not self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(numFileName)):\n",
    "            self.createNumMetadata(df, num_cols, numFileName)\n",
    "            self.createBoolMetadata(df, bool_cols, numFileName)####\n",
    "        else:\n",
    "            print(\"num metadata file exists for table {}\".format(name))\n",
    "        if not self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(timeFileName)):\n",
    "            self.createTimeMetadata(df, time_cols, timeFileName)\n",
    "        else:\n",
    "            print(\"timestamp metadata file exists for table {}\".format(name))\n",
    "\n",
    "    def createBoolMetadata(self, df, bool_cols, boolFilename):\n",
    "        for colName in bool_cols:\n",
    "            minMax = df.agg(f.min(df[colName]), f.max(df[colName])).collect()[0]\n",
    "            metaDf = self.sc.parallelize([\n",
    "                    (colName,float(minMax[0]),float(minMax[1]))]).toDF([\"colName\",\"min\",\"max\"])\n",
    "            metaDf.write.save(path=boolFilename, header=\"false\", format='csv', mode='append', sep = '^')\n",
    "            \n",
    "    def createTimeMetadata(self, df, time_cols, timeFileName):\n",
    "        for colName in time_cols:\n",
    "            minMax = df.agg(f.min(df[colName]), f.max(df[colName])).collect()[0]\n",
    "            metaDf = self.sc.parallelize([\n",
    "                    (colName,minMax[0].strftime(\"%Y-%m-%d %H:%M:%S\"),minMax[1].strftime(\"%Y-%m-%d %H:%M:%S\"))]).toDF([\"colName\",\"min\",\"max\"])\n",
    "            metaDf.write.save(path=timeFileName, header=\"false\", format='csv', mode='append', sep = '^')\n",
    "\n",
    "    def createNumMetadata(self, df, num_cols, numFileName):\n",
    "        describeTable = df[num_cols].describe().collect()\n",
    "        \n",
    "        for colName in num_cols:\n",
    "            metaDf = self.sc.parallelize([\n",
    "                     (colName,float(describeTable[3][colName]),float(describeTable[4][colName]))]).toDF([\"colName\",\"min\",\"max\"])\n",
    "            metaDf.write.save(path=numFileName, header=\"false\", format='csv', mode='append', sep = '^')\n",
    "\n",
    "    def timeColWithinRange(self, minTime, maxTime):\n",
    "        resultCreated = False\n",
    "        if type(minTime) != datetime.datetime or type(maxTime) != datetime.datetime:\n",
    "            raise TypeError(\"minNum, maxNum must be timestamp\")\n",
    "            \n",
    "        schema = StructType([\n",
    "            StructField(\"colName\", StringType(), True),\n",
    "            StructField(\"min\", TimestampType(), True),\n",
    "            StructField(\"max\", TimestampType(), True)])\n",
    "        \n",
    "        for each in self.tableNames:\n",
    "            filename = each + '_time_metadata.csv'\n",
    "            if self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(filename)):\n",
    "                currentTable = spark.read.csv(filename,header=False,schema=schema, sep='^')\n",
    "                if not resultCreated:\n",
    "                    resultDf = currentTable.where(currentTable.min>minTime).where(currentTable.max<maxTime).select(currentTable.colName).withColumn(\"tableName\", f.lit(each))\n",
    "                    resultCreated = True\n",
    "                else:\n",
    "                    resultDf = resultDf.union(currentTable.where(currentTable.min>minTime).where(currentTable.max<maxTime).select(currentTable.colName).withColumn(\"tableName\", f.lit(each)))\n",
    "\n",
    "        return resultDf\n",
    "\n",
    "    def numColWithinRange(self, minNum, maxNum):\n",
    "        # int, bigint, float, long\n",
    "        resultCreated = False\n",
    "        if type(minNum) == datetime.datetime or \\\n",
    "            type(minNum) == str or \\\n",
    "            type(maxNum) == datetime.datetime or \\\n",
    "            type(maxNum) == str:\n",
    "            raise TypeError(\"minNum, maxNum must be number\")\n",
    "            \n",
    "        schema = StructType([\n",
    "            StructField(\"colName\", StringType(), True),\n",
    "            StructField(\"min\", DoubleType(), True),\n",
    "            StructField(\"max\", DoubleType(), True)])\n",
    "        \n",
    "        for each in self.tableNames:\n",
    "            filename = each + '_num_metadata.csv'\n",
    "            if self.fs.exists(self.sc._jvm.org.apache.hadoop.fs.Path(filename)):\n",
    "                currentTable = spark.read.csv(filename,header=False,schema=schema, sep='^')\n",
    "                if not resultCreated:\n",
    "                    resultDf = currentTable.where(currentTable.min>minNum).where(currentTable.max<maxNum).select(currentTable.colName).withColumn(\"tableName\", f.lit(each))\n",
    "                    resultCreated = True\n",
    "                else:\n",
    "                    resultDf = resultDf.union(currentTable.where(currentTable.min>minNum).where(currentTable.max<maxNum).select(currentTable.colName).withColumn(\"tableName\",f.lit(each)))\n",
    "        return resultDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num metadata file exists for table open\n",
      "num metadata file exists for table parking\n",
      "timestamp metadata file exists for table parking\n",
      "+--------------+---------+\n",
      "|       colName|tableName|\n",
      "+--------------+---------+\n",
      "|summons_number|     open|\n",
      "|   fine_amount|     open|\n",
      "|summons_number|  parking|\n",
      "|violation_code|  parking|\n",
      "+--------------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|   colName|tableName|\n",
      "+----------+---------+\n",
      "|issue_date|  parking|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "tc.register(openTable, \"open\")\n",
    "tc.register(parkingTable, \"parking\")\n",
    "tc.numColWithinRange(0, 1000000000000).show()\n",
    "tc.timeColWithinRange(datetime.datetime(1994,1,1), datetime.datetime(2018,5,1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "dirname = '/user/jp4989/open_data_csv/'\n",
    "found = False\n",
    "with open('data_ids', 'r') as g:\n",
    "    ids = g.readlines()\n",
    "    for each in ids:\n",
    "        if each.strip() == '54k3-2wtq':\n",
    "            found = True\n",
    "        if not found:\n",
    "            continue\n",
    "        filename = dirname+each.strip()+'.csv'\n",
    "        df = spark.read.format('csv').options(header='true',inferschema='true').load(filename)\n",
    "        if not df.rdd.isEmpty():\n",
    "            tc.register(df, 'nyc_'+each.strip().replace('-','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TableCollections(spark, sc)\n",
    "dirname = '/user/jp4989/open_data_csv/'\n",
    "with open('data_ids', 'r') as g:\n",
    "    ids = g.readlines()\n",
    "    for each in ids:\n",
    "        tc.add_registered_table_name('nyc_'+each.strip().replace('-','_'))\n",
    "        if each.strip() == '6m56-5mfb':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             colName|    tableName|\n",
      "+--------------------+-------------+\n",
      "|City Council Dist...|nyc_22rf_yxcy|\n",
      "| Community Districts|nyc_22rf_yxcy|\n",
      "|  Borough Boundaries|nyc_22rf_yxcy|\n",
      "|    Police Precincts|nyc_22rf_yxcy|\n",
      "|         % Level 3+4|nyc_26kp_bgdh|\n",
      "|           % Level 2|nyc_26kp_bgdh|\n",
      "|           % Level 1|nyc_26kp_bgdh|\n",
      "|           % Level 3|nyc_26kp_bgdh|\n",
      "|  Average Class Size|nyc_276h_y36a|\n",
      "|          Level3+4_%|nyc_27h8_t3wt|\n",
      "|            Level2_%|nyc_27h8_t3wt|\n",
      "|            Level3_%|nyc_27h8_t3wt|\n",
      "|            district|nyc_27h8_t3wt|\n",
      "|            Level4_%|nyc_27h8_t3wt|\n",
      "|            Level1_%|nyc_27h8_t3wt|\n",
      "|            latitude|nyc_29km_avyc|\n",
      "|   Pct Level 3 and 4|nyc_2bh6_qmgg|\n",
      "|         Pct Level 3|nyc_2bh6_qmgg|\n",
      "|         Pct Level 4|nyc_2bh6_qmgg|\n",
      "|         Pct Level 2|nyc_2bh6_qmgg|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc.numColWithinRange(0, 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|  colName|    tableName|\n",
      "+---------+-------------+\n",
      "|longitude|nyc_29km_avyc|\n",
      "|Longitude|nyc_2fra_mtpn|\n",
      "|Longitude|nyc_2n4x_d97d|\n",
      "|Longitude|nyc_2q48_ip9a|\n",
      "|  Borough|nyc_35f6_8qd2|\n",
      "|Longitude|nyc_35sw_rdxj|\n",
      "|Longitude|nyc_36hn_wea6|\n",
      "|Longitude|nyc_37fm_7uaa|\n",
      "|Longitude|nyc_37it_gmcp|\n",
      "|Longitude|nyc_3qfc_4tta|\n",
      "|Longitude|nyc_3rfa_3xsf|\n",
      "|Longitude|nyc_3spy_rjpw|\n",
      "|Longitude|nyc_48pb_zy2g|\n",
      "|Longitude|nyc_4wf2_7kdu|\n",
      "|Longitude|nyc_4zdr_zwdi|\n",
      "|Longitude|nyc_56u5_n9sa|\n",
      "|Longitude|nyc_57mv_nv28|\n",
      "|Longitude|nyc_59kj_x8nc|\n",
      "|Longitude|nyc_59t5_r7nb|\n",
      "|Longitude|nyc_5e24_x4wa|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return NYC longitude columns\n",
    "tc.numColWithinRange(-76, -70).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "| colName|    tableName|\n",
      "+--------+-------------+\n",
      "|latitude|nyc_29km_avyc|\n",
      "|Latitude|nyc_2fra_mtpn|\n",
      "|Latitude|nyc_2n4x_d97d|\n",
      "|Latitude|nyc_2q48_ip9a|\n",
      "|Latitude|nyc_35sw_rdxj|\n",
      "|Latitude|nyc_36hn_wea6|\n",
      "|Latitude|nyc_37fm_7uaa|\n",
      "|Latitude|nyc_37it_gmcp|\n",
      "|Latitude|nyc_3qfc_4tta|\n",
      "|Latitude|nyc_3rfa_3xsf|\n",
      "|Latitude|nyc_3spy_rjpw|\n",
      "|Latitude|nyc_48pb_zy2g|\n",
      "|Latitude|nyc_4wf2_7kdu|\n",
      "|Latitude|nyc_4zdr_zwdi|\n",
      "|Latitude|nyc_56u5_n9sa|\n",
      "|Latitude|nyc_57mv_nv28|\n",
      "|Latitude|nyc_59kj_x8nc|\n",
      "|Latitude|nyc_59t5_r7nb|\n",
      "|Latitude|nyc_5e24_x4wa|\n",
      "|Latitude|nyc_5fn4_dr26|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return NYC latitude columns\n",
    "tc.numColWithinRange(39, 43).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
